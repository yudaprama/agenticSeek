# AgenticSeek : Alternative priv√©e et locale √† Manus.

<p align="center">
<img align="center" src="./media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo">
<p>

    [English](./README.md) | [‰∏≠Êñá](./README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](./README_CHT.md) | Fran√ßais | [Êó•Êú¨Ë™û](./README_JP.md) | [Portugu√™s (Brasil)](./README_PTBR.md)

*Une **alternative 100% locale √† Manus AI**, cet assistant vocal autonome navigue sur le web, √©crit du code et planifie des t√¢ches tout en gardant toutes les donn√©es sur votre appareil. Con√ßu pour les mod√®les de raisonnement locaux, il fonctionne enti√®rement sur votre mat√©riel, garantissant une confidentialit√© totale et aucune d√©pendance au cloud.*

[![Visiter AgenticSeek](https://img.shields.io/static/v1?label=Website&message=AgenticSeek&color=blue&style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![Licence](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Pourquoi AgenticSeek ?

* üîí 100% Local & Priv√© ‚Äì Tout fonctionne sur votre machine : pas de cloud, pas de partage de donn√©es. Vos fichiers, conversations et recherches restent priv√©s.

* üåê Navigation Web Intelligente ‚Äì AgenticSeek peut naviguer sur Internet de fa√ßon autonome : recherche, lecture, extraction d‚Äôinformations, remplissage de formulaires web ‚Äî tout cela sans intervention.

* üíª Assistant de Codage Autonome ‚Äì Besoin de code ? Il peut √©crire, d√©boguer et ex√©cuter des programmes en Python, C, Go, Java, et plus ‚Äî sans supervision.

* üß† S√©lection Intelligente d‚ÄôAgent ‚Äì Vous demandez, il choisit automatiquement le meilleur agent pour la t√¢che. Comme une √©quipe d‚Äôexperts √† disposition.

* üìã Planifie & Ex√©cute des T√¢ches Complexes ‚Äì De la planification de voyage √† la gestion de projets complexes : il divise les grandes t√¢ches en √©tapes et les r√©alise avec plusieurs agents IA.

* üéôÔ∏è Contr√¥le Vocal ‚Äì Voix et reconnaissance vocale rapides et futuristes, permettant de dialoguer comme avec une IA de film de science-fiction. (En d√©veloppement)

### **D√©mo**

> *Peux-tu rechercher le projet agenticSeek, d√©couvrir les comp√©tences requises, puis ouvrir le fichier CV_candidates.zip et me dire lesquels correspondent le mieux au projet ?*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Avertissement : Cette d√©mo, y compris tous les fichiers affich√©s (ex : CV_candidates.zip), est enti√®rement fictive. Nous ne sommes pas une entreprise, nous recherchons des contributeurs open-source, pas des candidats.

> üõ†‚ö†Ô∏èÔ∏è **Travail en cours**

> üôè Ce projet a commenc√© comme un projet annexe sans feuille de route ni financement. Il a d√©pass√© toutes nos attentes en finissant dans GitHub Trending. Les contributions, retours et votre patience sont grandement appr√©ci√©s.

## Pr√©requis

Assurez-vous d‚Äôavoir chrome driver, docker et python3.10 install√©s.

Pour les probl√®mes li√©s √† chrome driver, voir la section **Chromedriver**.

### 1. **Cloner le d√©p√¥t et configurer**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2. Modifier le contenu du fichier .env

```sh
SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optionnel'
DEEPSEEK_API_KEY='optionnel'
OPENROUTER_API_KEY='optionnel'
TOGETHER_API_KEY='optionnel'
GOOGLE_API_KEY='optionnel'
ANTHROPIC_API_KEY='optionnel'
```

**Les cl√©s API sont totalement optionnelles pour les utilisateurs qui choisissent d‚Äôex√©cuter le LLM localement. Ce qui est le but principal du projet. Laissez vide si vous avez le mat√©riel suffisant.**

Les variables d‚Äôenvironnement suivantes configurent les connexions et cl√©s API de votre application.

Mettez √† jour le fichier `.env` avec vos propres valeurs si besoin :

- **SEARXNG_BASE_URL** : Laisser inchang√©
- **REDIS_BASE_URL** : Laisser inchang√©
- **WORK_DIR** : Chemin vers votre dossier de travail local. AgenticSeek pourra lire et interagir avec ces fichiers.
- **OLLAMA_PORT** : Port pour le service Ollama.
- **LM_STUDIO_PORT** : Port pour le service LM Studio.
- **CUSTOM_ADDITIONAL_LLM_PORT** : Port pour tout service LLM personnalis√©.

Toutes les variables d‚Äôenvironnement de cl√© API ci-dessous sont **optionnelles**. Vous n‚Äôavez √† les fournir que si vous souhaitez utiliser des API externes au lieu d‚Äôex√©cuter les LLM localement.

### 3. **D√©marrer Docker**

Assurez-vous que Docker est install√© et en cours d‚Äôex√©cution sur votre syst√®me. Vous pouvez d√©marrer Docker avec les commandes suivantes :

- **Sur Linux/macOS :**  
        Ouvrez un terminal et lancez :
        ```sh
        sudo systemctl start docker
        ```
        Ou lancez Docker Desktop depuis votre menu d‚Äôapplications si install√©.

- **Sur Windows :**  
        Lancez Docker Desktop depuis le menu D√©marrer.

Vous pouvez v√©rifier que Docker fonctionne avec :
```sh
docker info
```
Si vous voyez des informations sur votre installation Docker, c‚Äôest que tout fonctionne.

---

## Configuration pour ex√©cuter un LLM localement

**Configuration mat√©rielle requise :**

Pour ex√©cuter des LLM localement, il vous faut un mat√©riel suffisant. Au minimum, un GPU capable d‚Äôex√©cuter Qwen/Deepseek 14B est requis. Voir la FAQ pour des recommandations d√©taill√©es.

**D√©marrer votre fournisseur local**

D√©marrez votre fournisseur local, par exemple avec ollama :

```sh
ollama serve
```

Voir ci-dessous la liste des fournisseurs locaux support√©s.

**Mettre √† jour le config.ini**

Modifiez le fichier config.ini pour d√©finir provider_name sur un fournisseur support√© et provider_model sur un mod√®le LLM support√© par votre fournisseur. Nous recommandons des mod√®les de raisonnement comme *Qwen* ou *Deepseek*.

Voir la **FAQ** √† la fin du README pour le mat√©riel requis.

```sh
[MAIN]
is_local = True # Si vous ex√©cutez localement ou avec un fournisseur distant.
provider_name = ollama # ou lm-studio, openai, etc.
provider_model = deepseek-r1:14b # choisissez un mod√®le adapt√© √† votre mat√©riel
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # nom de votre IA
recover_last_session = True # reprendre la session pr√©c√©dente
save_session = True # m√©moriser la session actuelle
speak = False # synth√®se vocale
listen = False # Reconnaissance vocale, uniquement pour CLI, exp√©rimental
jarvis_personality = False # Personnalit√© "Jarvis" (exp√©rimental)
languages = en zh # Liste des langues, la synth√®se vocale prendra la premi√®re par d√©faut
[BROWSER]
headless_browser = True # laisser inchang√© sauf si utilisation CLI sur l‚Äôh√¥te.
stealth_mode = True # Utilise selenium ind√©tectable pour r√©duire la d√©tection
```

**Attention** :

- Le format du fichier `config.ini` ne supporte pas les commentaires.
Ne copiez/collez pas la configuration d‚Äôexemple directement, car les commentaires provoqueront des erreurs. Modifiez manuellement le fichier `config.ini` avec vos param√®tres, sans commentaires.

- Ne mettez *pas* provider_name √† `openai` si vous utilisez LM-studio pour ex√©cuter les LLM. Mettez-le √† `lm-studio`.

- Certains fournisseurs (ex : lm-studio) n√©cessitent `http://` devant l‚ÄôIP. Par exemple `http://127.0.0.1:1234`

**Liste des fournisseurs locaux**

| Fournisseur  | Local ? | Description                                               |
|--------------|---------|----------------------------------------------------------|
| ollama       | Oui     | Ex√©cutez des LLM localement facilement avec ollama       |
| lm-studio    | Oui     | Ex√©cutez un LLM localement avec LM studio (`lm-studio`)  |
| openai       | Oui     | Utilise une API compatible openai (ex : serveur llama.cpp)|

√âtape suivante : [D√©marrer les services et lancer AgenticSeek](#Start-services-and-Run)

*Voir la section **Probl√®mes connus** en cas de souci*

*Voir la section **Utiliser une API** si votre mat√©riel ne peut pas ex√©cuter deepseek localement*

*Voir la section **Config** pour une explication d√©taill√©e du fichier de configuration.*

---

## Configuration pour utiliser une API

**L‚Äôutilisation d‚Äôune API est optionnelle, voir ci-dessus pour l‚Äôex√©cution locale.**

D√©finissez le fournisseur d√©sir√© dans le `config.ini`. Voir ci-dessous la liste des fournisseurs API.

```sh
[MAIN]
is_local = False
provider_name = google
provider_model = gemini-2.0-flash
provider_server_address = 127.0.0.1:5000 # sans importance
```
Attention : Ne laissez pas d‚Äôespace √† la fin de la config.

Exportez votre cl√© API : `export <<PROVIDER>>_API_KEY="xxx"`

Exemple : export `TOGETHER_API_KEY="xxxxx"`

**Liste des fournisseurs API**
    
| Fournisseur  | Local ? | Description                                               |
|--------------|---------|----------------------------------------------------------|
| openai       | Selon   | Utilise l‚ÄôAPI ChatGPT  |
| deepseek     | Non     | API Deepseek (non priv√©)                            |
| huggingface  | Non     | API Hugging-Face (non priv√©)                            |
| togetherAI   | Non     | Utilise l‚ÄôAPI together AI (non priv√©)                   |
| google       | Non     | Utilise l‚ÄôAPI gemini de Google (non priv√©)              |

Notez que le codage/bash peut √©chouer avec gemini, qui ignore parfois le format demand√©, optimis√© pour deepseek r1. Les mod√®les comme gpt-4o donnent aussi de moins bons r√©sultats avec notre prompt.

√âtape suivante : [D√©marrer les services et lancer AgenticSeek](#Start-services-and-Run)

*Voir la section **Probl√®mes connus** en cas de souci*

*Voir la section **Config** pour une explication d√©taill√©e du fichier de configuration.*

---

## D√©marrer les services et lancer AgenticSeek

D√©marrez les services requis. Cela lancera tous les services du docker-compose.yml, dont :
        - searxng
        - redis (requis par searxng)
        - frontend
        - backend (si vous utilisez `full`)

```sh
./start_services.sh full # MacOS
start ./start_services.cmd full # Windows
```

**Attention :** Cette √©tape t√©l√©chargera et chargera toutes les images Docker, ce qui peut prendre jusqu‚Äô√† 30 minutes. Apr√®s le d√©marrage, attendez que le backend soit bien lanc√© (vous devriez voir backend: <info> dans les logs) avant d‚Äôenvoyer des messages. Le backend peut mettre plus de temps √† d√©marrer.

Allez sur `http://localhost:3000/` pour acc√©der √† l‚Äôinterface web.

**Optionnel : Utiliser l‚Äôinterface CLI :**

Pour utiliser le mode CLI, vous devrez installer les paquets sur l‚Äôh√¥te :

```sh
./install.sh
./install.bat # windows
```

D√©marrez les services :

```sh
./start_services.sh # MacOS
start ./start_services.cmd # Windows
```

Puis lancez : `python3 cli.py`

---

## Utilisation

Assurez-vous que les services sont lanc√©s avec `./start_services.sh full` et allez sur `localhost:3000` pour l‚Äôinterface web.

Vous pouvez aussi utiliser la reconnaissance vocale en mettant `listen = True` dans la config (mode CLI uniquement).

Pour quitter, dites/tapez simplement `goodbye`.

Exemples d‚Äôutilisation :

> *Fais un jeu du serpent en python !*

> *Recherche les meilleurs caf√©s √† Rennes, France, et enregistre une liste de trois avec leurs adresses dans rennes_cafes.txt.*

> *√âcris un programme Go pour calculer la factorielle d‚Äôun nombre, sauvegarde-le sous factorial.go dans ton espace de travail*

> *Cherche dans mon dossier summer_pictures tous les fichiers JPG, renomme-les avec la date du jour, et enregistre la liste dans photos_list.txt*

> *Recherche en ligne les films de science-fiction populaires de 2024 et choisis-en trois √† regarder ce soir. Sauvegarde la liste dans movie_night.txt.*

> *Recherche les derniers articles d‚Äôactualit√© sur l‚ÄôIA de 2025, s√©lectionne-en trois, et √©cris un script Python pour extraire leurs titres et r√©sum√©s. Sauvegarde le script sous news_scraper.py et les r√©sum√©s dans ai_news.txt dans /home/projects*

> *Vendredi, cherche une API gratuite de prix d‚Äôactions, inscris-toi avec supersuper7434567@gmail.com puis √©cris un script Python pour r√©cup√©rer les prix quotidiens de Tesla, et sauvegarde les r√©sultats dans stock_prices.csv*

*Note : le remplissage de formulaires est encore exp√©rimental et peut √©chouer.*

Apr√®s avoir saisi votre requ√™te, AgenticSeek choisira le meilleur agent pour la t√¢che.

Comme il s‚Äôagit d‚Äôun prototype, le syst√®me de routage d‚Äôagent peut ne pas toujours choisir le bon agent selon votre requ√™te.

Soyez donc explicite sur ce que vous voulez et comment l‚ÄôIA doit proc√©der. Par exemple, pour une recherche web, ne dites pas :

`Connais-tu de bons pays pour voyager en solo ?`

Dites plut√¥t :

`Fais une recherche web et trouve les meilleurs pays pour voyager en solo`

---

## **Ex√©cuter le LLM sur votre propre serveur**

Si vous avez un ordinateur ou un serveur puissant, mais souhaitez l‚Äôutiliser depuis votre laptop, vous pouvez ex√©cuter le LLM sur un serveur distant via notre serveur LLM personnalis√©.

Sur votre "serveur" qui ex√©cutera le mod√®le IA, r√©cup√©rez l‚Äôadresse IP :

```sh
ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # ip locale
curl https://ipinfo.io/ip # ip publique
```

Note : Sous Windows ou macOS, utilisez ipconfig ou ifconfig pour trouver l‚Äôadresse IP.

Clonez le d√©p√¥t et entrez dans le dossier `server/` :

```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Installez les d√©pendances sp√©cifiques au serveur :

```sh
pip3 install -r requirements.txt
```

Lancez le script serveur :

```sh
python3 app.py --provider ollama --port 3333
```

Vous pouvez choisir entre `ollama` et `llamacpp` comme service LLM.

Sur votre ordinateur personnel :

Modifiez le fichier `config.ini` pour mettre `provider_name` √† `server` et `provider_model` √† `deepseek-r1:xxb`.
Mettez `provider_server_address` √† l‚Äôadresse IP de la machine qui ex√©cute le mod√®le.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
```

√âtape suivante : [D√©marrer les services et lancer AgenticSeek](#Start-services-and-Run)

---

## Reconnaissance Vocale

Attention : la reconnaissance vocale ne fonctionne qu‚Äôen mode CLI pour l‚Äôinstant.

Notez qu‚Äôactuellement la reconnaissance vocale ne fonctionne qu‚Äôen anglais.

La fonctionnalit√© de reconnaissance vocale est d√©sactiv√©e par d√©faut. Pour l‚Äôactiver, mettez listen √† True dans le fichier config.ini :

```
listen = True
```

Quand activ√©e, la reconnaissance vocale attend un mot-cl√© d√©clencheur, qui est le nom de l‚Äôagent, avant de commencer √† traiter votre entr√©e. Vous pouvez personnaliser le nom de l‚Äôagent via `agent_name` dans *config.ini* :

```
agent_name = Friday
```

Pour une reconnaissance optimale, nous recommandons d‚Äôutiliser un pr√©nom anglais courant comme "John" ou "Emma" comme nom d‚Äôagent.

Une fois la transcription affich√©e, dites le nom de l‚Äôagent √† voix haute pour le r√©veiller (ex : "Friday").

√ânoncez clairement votre requ√™te.

Terminez votre demande par une phrase de confirmation pour signaler au syst√®me de proc√©der. Exemples :
```
"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
```

## Config

Exemple de config :
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
jarvis_personality = False
languages = en zh
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explications** :

- is_local -> Ex√©cute l‚Äôagent localement (True) ou sur un serveur distant (False).

- provider_name -> Fournisseur √† utiliser (parmi : `ollama`, `server`, `lm-studio`, `deepseek-api`)

- provider_model -> Mod√®le utilis√©, ex : deepseek-r1:32b.

- provider_server_address -> Adresse du serveur, ex : 127.0.0.1:11434 pour local. Mettre n‚Äôimporte quoi pour une API non locale.

- agent_name -> Nom de l‚Äôagent, ex : Friday. Utilis√© comme mot-cl√© pour la reconnaissance vocale.

- recover_last_session -> Reprend la derni√®re session (True) ou non (False).

- save_session -> Sauvegarde la session (True) ou non (False).

- speak -> Active la synth√®se vocale (True) ou non (False).

- listen -> Active la reconnaissance vocale (True) ou non (False).

- jarvis_personality -> Utilise une personnalit√© type JARVIS (True) ou non (False). Change simplement le prompt.

- languages -> Liste des langues support√©es, n√©cessaire pour le routage des agents. √âvitez d‚Äôen mettre trop ou des langues trop similaires.

- headless_browser -> Lance le navigateur sans fen√™tre visible (True) ou non (False).

- stealth_mode -> Rend la d√©tection par les bots plus difficile. N√©cessite d‚Äôinstaller l‚Äôextension anticaptcha manuellement.

- languages -> Liste des langues support√©es. Requis pour le syst√®me de routage. Plus la liste est longue, plus de mod√®les seront t√©l√©charg√©s.

## Fournisseurs

Tableau des fournisseurs disponibles :

| Fournisseur  | Local ? | Description                                               |
|--------------|---------|----------------------------------------------------------|
| ollama       | Oui     | Ex√©cutez des LLM localement avec ollama                  |
| server       | Oui     | H√©bergez le mod√®le sur une autre machine                 |
| lm-studio    | Oui     | Ex√©cutez un LLM localement avec LM studio (`lm-studio`)  |
| openai       | Selon   | Utilise l‚ÄôAPI ChatGPT (non priv√©) ou API compatible openai|
| deepseek-api | Non     | API Deepseek (non priv√©)                                 |
| huggingface  | Non     | API Hugging-Face (non priv√©)                             |
| togetherAI   | Non     | Utilise l‚ÄôAPI together AI (non priv√©)                    |
| google       | Non     | Utilise l‚ÄôAPI gemini de Google (non priv√©)               |

Pour s√©lectionner un fournisseur, modifiez le config.ini :

```
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:5000
```
`is_local` : doit √™tre True pour tout LLM local, sinon False.

`provider_name` : S√©lectionnez le fournisseur par son nom, voir la liste ci-dessus.

`provider_model` : Mod√®le √† utiliser par l‚Äôagent.

`provider_server_address` : Peut √™tre n‚Äôimporte quoi si vous n‚Äôutilisez pas le fournisseur server.

# Probl√®mes connus

## Probl√®mes avec Chromedriver

**Erreur connue #1 :** *chromedriver mismatch*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

Cela arrive s‚Äôil y a un d√©calage entre la version de votre navigateur et celle de chromedriver.

T√©l√©chargez la derni√®re version ici :

https://developer.chrome.com/docs/chromedriver/downloads

Si vous utilisez Chrome version 115 ou plus, allez sur :

https://googlechromelabs.github.io/chrome-for-testing/

Et t√©l√©chargez la version de chromedriver correspondant √† votre OS.

![alt text](./media/chromedriver_readme.png)

Si cette section est incompl√®te, ouvrez une issue.

## Probl√®mes d‚Äôadaptateurs de connexion

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:11434/v1/chat/completions'
```

Assurez-vous d‚Äôavoir `http://` devant l‚Äôadresse IP du fournisseur :

`provider_server_address = http://127.0.0.1:11434`

## SearxNG base URL doit √™tre fourni

```
raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.
```

Peut-√™tre n‚Äôavez-vous pas renomm√© `.env.example` en `.env` ? Vous pouvez aussi exporter SEARXNG_BASE_URL :

`export  SEARXNG_BASE_URL="http://127.0.0.1:8080"`

## FAQ

**Q : Quel mat√©riel est n√©cessaire ?**  

| Taille du mod√®le | GPU           | Commentaire                                                                 |
|------------------|---------------|-----------------------------------------------------------------------------|
| 7B               | 8 Go Vram     | ‚ö†Ô∏è Non recommand√©. Performances faibles, hallucinations fr√©quentes, √©chec probable des agents planificateurs. |
| 14B              | 12 Go VRAM    | ‚úÖ Utilisable pour des t√¢ches simples. Peut avoir du mal avec la navigation web et la planification. |
| 32B              | 24+ Go VRAM   | üöÄ R√©ussite sur la plupart des t√¢ches, peut encore avoir du mal avec la planification |
| 70B+             | 48+ Go Vram   | üí™ Excellent. Recommand√© pour les cas avanc√©s.                              |

**Q : Pourquoi Deepseek R1 plut√¥t qu‚Äôun autre mod√®le ?**  

Deepseek R1 excelle en raisonnement et utilisation d‚Äôoutils pour sa taille. Nous pensons que c‚Äôest un bon choix, d‚Äôautres mod√®les fonctionnent aussi, mais Deepseek est notre favori.

**Q : J‚Äôai une erreur en lan√ßant `cli.py`. Que faire ?**  

Assurez-vous que le local est lanc√© (`ollama serve`), que votre `config.ini` correspond √† votre fournisseur, et que les d√©pendances sont install√©es. Si rien ne marche, ouvrez une issue.

**Q : Peut-on vraiment tout faire tourner en local ?**  

Oui, avec Ollama, lm-studio ou server, tout (reconnaissance vocale, LLM, synth√®se vocale) fonctionne localement. Les options non-locales (OpenAI ou autres API) sont optionnelles.

**Q : Pourquoi utiliser AgenticSeek alors que j‚Äôai Manus ?**

Ce projet est n√© d‚Äôun int√©r√™t pour les agents IA. Ce qui le rend sp√©cial, c‚Äôest la volont√© d‚Äôutiliser des mod√®les locaux et d‚Äô√©viter les API.
Nous nous inspirons de Jarvis et Friday (Iron Man) pour le c√¥t√© "cool", mais pour la fonctionnalit√©, c‚Äôest Manus qui nous inspire, car c‚Äôest ce que les gens recherchent : une alternative locale √† Manus.
Contrairement √† Manus, AgenticSeek privil√©gie l‚Äôind√©pendance vis-√†-vis des syst√®mes externes, vous donnant plus de contr√¥le, de confidentialit√© et √©vitant les co√ªts d‚ÄôAPI.

## Contribuer

Nous recherchons des d√©veloppeurs pour am√©liorer AgenticSeek ! Consultez les issues ou discussions ouvertes.

[Guide de contribution](./docs/CONTRIBUTING.md)

[![Star History Chart](https://api.star-history.com/svg?repos=Fosowl/agenticSeek&type=Date)](https://www.star-history.com/#Fosowl/agenticSeek&Date)

## Mainteneurs :

 > [Fosowl](https://github.com/Fosowl) | Heure de Paris 

 > [antoineVIVIES](https://github.com/antoineVIVIES) | Heure de Taipei 

 > [steveh8758](https://github.com/steveh8758) | Heure de Taipei 

## Remerciements :

 > [tcsenpai](https://github.com/tcsenpai) et [plitc](https://github.com/plitc) pour l‚Äôaide √† la dockerisation du backend

